{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidataparsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidataparsing.wikidataparsing import DARES\n",
    "\n",
    "from model.model import SyntacticIndex, SemanticIndex\n",
    "\n",
    "from utils.utils import balanceRelationDataset, loadCorpus, prepare_corpus\n",
    "from collections import Counter\n",
    "import json \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg\n",
    "!python -m spacy download fr_core_news_lg\n",
    "!python -m spacy download en_core_web_trf\n",
    "!python -m spacy download fr_dep_news_trf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the DARES dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language code to select the language of the corpus\n",
    "lg = 'en'\n",
    "# lg = 'fr'\n",
    "\n",
    "# nlp = spacy.load('fr_dep_news_trf')\n",
    "# nlp = spacy.load('en_core_web_trf')\n",
    "# nlp = spacy.load('fr_core_news_lg')\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\n",
    "# specifies number of cores to use\n",
    "n_core = 6\n",
    "\n",
    "\n",
    "wp = DARES(lg=lg, nlp_model='en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rel = [\n",
    "    {\n",
    "        # specifiy by what Item type / Property must search in the WhatLinksHere pages (e.g. Q5 ('human'))\n",
    "        \"type\": 'Q5',\n",
    "        # you can provide a label for the Item / Property (e.g. 'human')\n",
    "        \"name\": \"human\",\n",
    "        # indicate the set of relations you want to collect from Wikidata / Wikipedia\n",
    "        \"props\":{\n",
    "            # PXX are the identifier of a Property on Wikidata\n",
    "            # you define the value (e.g. placeOfBirth)\n",
    "            \"P19\": 'placeOfBirth',\n",
    "            \"P569\": \"dateOfBirth\",\n",
    "            \"P509\": 'causeOfDeath',\n",
    "            \"P570\": \"dateOfDeath\",\n",
    "            \"P119\": \"placeOfBurial\",\n",
    "            \"P26\": \"spouse\",\n",
    "            \"P106\": \"occupation\",\n",
    "            \"P69\": \"educatedAt\",\n",
    "        }      \n",
    "    }]\n",
    "\n",
    "projectname = f'Q5'\n",
    "\n",
    "wp.initiate_project(projectname, dict_rel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below, example when collecting data for multiple entity types (here, person and location)\n",
    "dict_rel = [\n",
    "    {\n",
    "        \"type\": 'Q5',\n",
    "        \"name\": \"person\",\n",
    "        \"props\":{\n",
    "            \"P19\": 'placeOfBirth',\n",
    "            \"P569\": \"dateOfBirth\",\n",
    "            \"P509\": 'causeOfDeath',\n",
    "            \"P570\": \"dateOfDeath\",\n",
    "            \"P119\": \"placeOfBurial\",\n",
    "            \"P26\": \"spouse\",\n",
    "            \"P106\": \"occupation\",\n",
    "            \"P69\": \"educatedAt\",\n",
    "            \"P509\": \"causeOfDeath\"\n",
    "        }      \n",
    "    },\n",
    "    {   \n",
    "        \"type\": \"Property:P625\",\n",
    "        \"name\": \"location\",\n",
    "        'props':{\n",
    "            \"P571\": 'inception',\n",
    "            \"P17\": \"country\",\n",
    "            \"P1082\": \"population\",\n",
    "            \"P1376\": \"capitalOf\",\n",
    "            \"P276\": \"location\",\n",
    "            \"P36\":\"capital\",\n",
    "            \"P35\": \"headOfState\",\n",
    "            \"P6\": \"headOfGoverment\",\n",
    "            \"P1082\": \"population\",\n",
    "            \"P47\": \"sharesBordersWith\",\n",
    "            \"P463\": \"memberOf\",\n",
    "            \"P206\": \"nextInBodyWater\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "projectname = f'test3'\n",
    "\n",
    "wp.initiate_project(projectname, dict_rel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total of pages to collect\n",
    "limit = 1\n",
    "# limit = 10\n",
    "\n",
    "# specifies at which steps saves collected entity ids to disk\n",
    "save_step = 20\n",
    "\n",
    "# specifies how many ids to collect per pages\n",
    "m_size = 10\n",
    "\n",
    "# m_size = 50\n",
    "\n",
    "# collect list of Wikidata entities\n",
    "wp.collect_Wikidata_links(dict_rel, limit, m_size, save_step, n_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_doc = 'wikipedia'\n",
    "\n",
    "# similarity threshold for the distant supervision step\n",
    "score_cutoff = 95\n",
    "\n",
    "# getOther = False\n",
    "getOther = True\n",
    "maxsizesent = True\n",
    "\n",
    "wp.processListEntities(source_doc=source_doc, score_cutoff=score_cutoff, getOther=getOther, maxsizesent=maxsizesent, n_core=n_core)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest Dependency Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# removes sentences where these is no entity match\n",
    "removeNoMatch = True\n",
    "\n",
    "# list of Property by ID to keep. Here, keeps every Property\n",
    "\n",
    "corpus = wp.extract_sdp(removeNoMatch=removeNoMatch, n_core=n_core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks the distribution of relation in dataset\n",
    "list_prop = [z['prop'] for x in corpus for y in x['content'] for z in y['props']]\n",
    "# list_prop\n",
    "Counter(list_prop).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "\n",
    "\n",
    "# use this cell if you want to define the type of the target entitys involved in each Relation\n",
    "def replaceEntType(dict_prop:dict) -> dict:\n",
    "    prop = dict_prop['prop']\n",
    "\n",
    "    if prop == 'P19': # placeOfBirth\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "    elif prop == 'P569': # dateOfBirth\n",
    "        dict_prop['target_type'] = 'time'\n",
    "\n",
    "    elif prop == 'P570': # dateOfDeath\n",
    "        dict_prop['target_type'] = 'time'\n",
    "\n",
    "    elif prop == 'P26': # spouse\n",
    "        dict_prop['target_type'] = 'Q5'\n",
    "\n",
    "    elif prop == 'P106': # occupation\n",
    "        dict_prop['target_type'] = 'Misc'\n",
    "\n",
    "    elif prop == 'P69': # educatedAt\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "    elif prop == 'P17': # country\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "    elif prop == 'P571': # inception\n",
    "        dict_prop['target_type'] = 'time'\n",
    "\n",
    "    elif prop == 'P1376': # capitalOf\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "    elif prop == 'P36': # capital\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "    elif prop == 'P6': # headOfGoverment\n",
    "        dict_prop['target_type'] = 'Q5'\n",
    "\n",
    "    elif prop == 'P47': # sharesBordersWith\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "    elif prop == 'P463': # memberOf\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "    elif prop == 'P206': # nextInBodyWater\n",
    "        dict_prop['target_type'] = 'Property:P625'\n",
    "\n",
    "\n",
    "    return dict_prop\n",
    "\n",
    "for filepath in glob(f\"{project}/corpus/**/*.json\", recursive=True):\n",
    "    print(filepath)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    try:\n",
    "        for sent in data['content']:\n",
    "            for prop in sent['props']:\n",
    "                prop = replaceEntType(prop)\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            data = json.dump(data, f, indent=4)\n",
    "    except:\n",
    "        print(filepath)\n",
    "    # break\n",
    "# with open(f\"{project}/corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean = True if you want to removes sentences annotated as Other\n",
    "clean = True\n",
    "corpus = loadCorpus(wp.project_path, clean=clean)\n",
    "len(corpus), corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to convert the corpus in a format for building the indices\n",
    "# also, allows to divide the corpus into a train, dev and validation test\n",
    "params = {\n",
    "    # corpus to process\n",
    "    \"corpus\": corpus,\n",
    "    # size of the train set. 1 means the whole corpus\n",
    "    \"train_size\": 1,\n",
    "    # size of the dev set\n",
    "    \"dev_size\": 0,\n",
    "    # removes Other labels\n",
    "    \"clean\": True\n",
    "    # \"maxsize\": 200000\n",
    "}\n",
    "data = prepare_corpus(**params)\n",
    "# print(len(list_graphs))\n",
    "# list_graphs[0]\n",
    "try:\n",
    "    print(len(data['X_train']), len(data['X_dev']), len(data['X_test']))\n",
    "except:\n",
    "    print(len(data['X_train']), len(data['y_train']))\n",
    "\n",
    "c = Counter([y for y in data['y_train']])\n",
    "print(pd.Series(dict(c.most_common())).to_frame().to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to build the Syntactic Index \n",
    "syntactic_index_params = {\n",
    "    # data to use for building the index\n",
    "    \"list_graphs\": data['X_train'],\n",
    "    # surface form of the predicates\n",
    "    \"anchor_textvalue\": ['lemma', 'pos'],\n",
    "    # which graph to use as lexico-syntactic pattern\n",
    "    \"graphkey\": 'sdpgraph',\n",
    "    # key to use for relation label\n",
    "    \"propkey\": 'prop',\n",
    "    # minimum support for each relation per pattern\n",
    "    \"support\": 0,\n",
    "    \"dict_rel\": dict_rel,\n",
    "    \"savepath\": wp.project_path\n",
    "}\n",
    "syntacticIndex = SyntacticIndex()\n",
    "syntacticIndex.trainSyntacticIndex(**syntactic_index_params)\n",
    "syntacticIndex.syntacticIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_index_params = {\n",
    "    \"list_graphs\": data['X_train'],\n",
    "    \"textvalue\":  syntacticIndex.syntacticIndexParams['anchor_textvalue'],\n",
    "    \"dict_rel\": dict_rel,\n",
    "    \"removePROPN\": True,\n",
    "    \"savepath\": wp.project_path\n",
    "\n",
    "}\n",
    "base_semanticIndex = SemanticIndex()\n",
    "base_semanticIndex.trainSemanticIndex(**semantic_index_params)\n",
    "base_semanticIndex.semanticIndex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elijere",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5889619cf79a50ffdba942884c499303b51d6ff1a84c2173e8fdab8898557e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
